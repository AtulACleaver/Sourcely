# âš™ï¸ Sourcely Backend

FastAPI server for document processing and RAG-based questioning.

## ğŸš€ Setup

1. **python environment**:

   ```bash
   python -m venv venv
   source venv/bin/activate  # on windows: venv\Scripts\activate
   ```

2. **install dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

3. **configure environment**:

   Create a `.env` file:

   ```env
   OLLAMA_BASE_URL=http://localhost:11434
   GENERATIONAL_MODEL=llama3
   ```

4. **run the server**:

   ```bash
   python main.py
   ```

## ğŸ” requirements

- **ollama**: ensure ollama is running locally.
- **models**: pull the required models:

  ```bash
  ollama pull nomic-embed-text
  ollama pull llama3
  ```

## ğŸ”Œ endpoints

- `POST /upload`: upload and index a pdf.
- `POST /query`: ask questions about indexed documents.
- `GET /status`: check the current index state.

## ğŸ“ Project Structure

```text
backend/
â”œâ”€â”€ main.py             # fastapi entry point & routes
â”œâ”€â”€ extraction.py       # pdf text extraction logic
â”œâ”€â”€ chunking.py         # text segmentation strategies
â”œâ”€â”€ embeddings.py       # faiss index & embedding utilities
â”œâ”€â”€ generation.py       # rag logic & llm interaction
â”œâ”€â”€ requirements.txt    # python dependencies
â”œâ”€â”€ .env                # environment variables (local only)
â”œâ”€â”€ uploads/            # temporary storage for pdfs
â””â”€â”€ vector_store/       # faiss index & chunk metadata
```
