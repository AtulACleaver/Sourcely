# ğŸ“„ Sourcely - RAG-Powered PDF Research Assistant

An AI-powered research assistant that allows users to upload PDF documents and ask questions about their content. Sourcely uses Retrieval-Augmented Generation (RAG) to provide accurate answers with direct citations from the uploaded source.

![Sourcely](https://img.shields.io/badge/Sourcely-v1.0.0-blue)
![Python](https://img.shields.io/badge/Python-3.10+-blue)
![React](https://img.shields.io/badge/React-19-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.129-green)
![FAISS](https://img.shields.io/badge/FAISS-1.13.2-yellow)

## âœ¨ Features

- **ğŸ“‚ PDF Upload & Processing**: Seamlessly upload and extract text from local PDF files.
- **âœ‚ï¸ Intelligent Chunking**: Automatically breaks down large documents into manageable segments for precise retrieval.
- **ğŸ” Vector-Based Search**: Uses FAISS for high-performance similarity search within document embeddings.
- **ğŸ’¬ RAG Question Answering**: Get LLM-generated answers based strictly on the context of your uploaded documents.
- **ğŸ“ Precise Citations**: Every answer includes citations pointing back to the specific chunks used for the response.
- **âš¡ Real-time Feedback**: Live status updates during the PDF processing pipeline.

## ğŸ¯ How It Works

### The Sourcely Processing Pipeline

```mermaid
graph TD
    A[User Uploads PDF] --> B[Text Extraction]
    B --> C[Text Chunking]
    C --> D[Embedding Generation]
    D --> E[FAISS Index Building]
    E --> F[Ready for Queries]
    
    G[User Asks Question] --> H[Question Embedding]
    H --> I[Similarity Search in FAISS]
    I --> J[Retrieve Relevant Chunks]
    J --> K[LLM Response Generation]
    K --> L[Answer with Citations]
    
    subgraph "PDF Ingestion"
    B
    C
    D
    E
    end
    
    subgraph "Query Handling"
    H
    I
    J
    K
    end
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Node.js 18+
- Ollama (running locally with your preferred model, e.g., llama3)

### Backend Setup

1. **Navigate to the backend directory:**
   ```bash
   cd backend
   ```

2. **Create and activate a virtual environment:**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment variables:**
   Create a `.env` file in the `backend/` directory:
   ```env
   # Add any required configuration here
   ```

5. **Run the FastAPI server:**
   ```bash
   python main.py
   ```
   The API will be available at `http://localhost:8000`

### Frontend Setup

1. **Navigate to the frontend directory:**
   ```bash
   cd frontend
   ```

2. **Install dependencies:**
   ```bash
   npm install
   ```

3. **Run the development server:**
   ```bash
   npm run dev
   ```
   The application will be available at `http://localhost:5173`

## ğŸ“ Project Structure

```text
Sourcely/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py             # FastAPI entry point & routes
â”‚   â”œâ”€â”€ extraction.py       # PDF text extraction logic
â”‚   â”œâ”€â”€ chunking.py         # Text segmentation strategies
â”‚   â”œâ”€â”€ embeddings.py       # FAISS index & embedding utilities
â”‚   â”œâ”€â”€ generation.py       # RAG logic & LLM interaction
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â””â”€â”€ uploads/            # Temporary storage for PDFs
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx         # Main UI & routing
â”‚   â”‚   â”œâ”€â”€ components/     # Reusable UI components
â”‚   â”‚   â”œâ”€â”€ api/            # API client services
â”‚   â”‚   â””â”€â”€ assets/         # Images & styles
â”‚   â”œâ”€â”€ package.json        # Frontend dependencies
â”‚   â””â”€â”€ vite.config.js      # Vite configuration
â””â”€â”€ README.md
```

## ğŸ”Œ API Endpoints

| Method | Endpoint | Description |
| :--- | :--- | :--- |
| GET | `/health` | Check if the service is running |
| GET | `/status` | Check the current status of the index |
| POST | `/upload` | Upload and process a PDF file |
| POST | `/query` | Ask a question based on indexed content |

## ğŸ’» Tech Stack

- **Backend**: FastAPI, PDFMiner, FAISS, LangChain (or custom RAG logic)
- **Frontend**: React 19, Vite, Tailwind CSS, Axios
- **LLM**: Powered by Ollama (local inference)

## ğŸ’¡ Troubleshooting

- **PDF Extraction Failed**: Ensure the PDF is not password-protected or purely image-based (OCR not currently supported).
- **Ollama Connection Error**: Verify that Ollama is running (`ollama serve`) and the model is pulled (`ollama pull llama3`).
- **Memory Issues**: Large PDFs may require significant RAM for embedding generation.

## ğŸ“ License

MIT License - feel free to use and modify!

---

**Note**: This project is designed for local research and document analysis. Ensure you have the rights to the documents you upload.
